{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA CLEANING\n",
        "\n",
        "This notebook contains majority of the code that we used to clean and fill the NaN values in our dataset. You can find more detailed explanations of the columns we manipulated below."
      ],
      "metadata": {
        "id": "gmFba-5LYfhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contextily\n",
        "import contextily as ctx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi-JmRSZq2k-",
        "outputId": "f09260fe-25a4-46d8-a795-652e0d67f1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contextily\n",
            "  Downloading contextily-1.6.2-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (from contextily) (2.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from contextily) (3.10.0)\n",
            "Collecting mercantile (from contextily)\n",
            "  Downloading mercantile-1.2.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from contextily) (11.1.0)\n",
            "Collecting rasterio (from contextily)\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from contextily) (2.32.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from contextily) (1.4.2)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.11/dist-packages (from contextily) (2025.1.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy->contextily) (2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->contextily) (2.8.2)\n",
            "Requirement already satisfied: click>=3.0 in /usr/local/lib/python3.11/dist-packages (from mercantile->contextily) (8.1.8)\n",
            "Collecting affine (from rasterio->contextily)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio->contextily) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio->contextily) (2025.1.31)\n",
            "Collecting cligj>=0.5 (from rasterio->contextily)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio->contextily)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->contextily) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->contextily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->contextily) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->contextily) (1.17.0)\n",
            "Downloading contextily-1.6.2-py3-none-any.whl (17 kB)\n",
            "Downloading mercantile-1.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: mercantile, cligj, click-plugins, affine, rasterio, contextily\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 contextily-1.6.2 mercantile-1.2.1 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrqBlwu0AQM7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import geopandas as gpd\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from shapely.geometry import Point\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVef1rGrq9f0",
        "outputId": "04b897bd-b10d-4173-dbd0-1b63f6c0973a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/Shareddrives/Project 3 (Arrest Dataset)/Arrest_Data_from_2020_to_Present.csv')"
      ],
      "metadata": {
        "id": "9SJVQwoJAeIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**\n",
        "\n",
        "This text cell explains how we dealt with each of the columns that had missing values.\n",
        "\n",
        "## Cross Street\n",
        "\n",
        "- Given that this value is missing in over half of the data, we decided to drop the column. We already have plenty of data regarding location anyways.\n",
        "\n",
        "## Booking Date\n",
        "\n",
        "- First filled missing Booking Dates with the Arrest Date\n",
        "- Then analyzed time differences to identify overnight bookings (found 16.1% of bookings cross to the next day)\n",
        "- Automatically adjusted 6,576 booking dates for overnight bookings\n",
        "- Fixed 24 inconsistencies where booking time was earlier than arrest time but dates were the same\n",
        "\n",
        "## Booking Time\n",
        "\n",
        "- Calculated the median time difference between arrest and booking (157 minutes)\n",
        "- Successfully filled all 75,378 missing booking times using this pattern\n",
        "- Properly handled cases where bookings cross into the next day\n",
        "- Converted properly to military time format stored as floats\n",
        "\n",
        "## Booking Location\n",
        "\n",
        "- Used a 2 step approach to fill missing Booking Locations:\n",
        "  1. First filled 72,932 locations using Address → Booking Location mapping\n",
        "  2. Filled additional 2,487 locations using Area Name → Booking Location mapping\n",
        "\n",
        "## Booking Location Code\n",
        "\n",
        "- Created mapping from Booking Location → Booking Location Code\n",
        "- Successfully filled all 75,382 missing Booking Location Codes\n",
        "- Achieved 100% completion for this column\n",
        "\n",
        "## Arrest Type Code\n",
        "\n",
        "- Dropped 22 rows with missing Arrest Type Code values\n",
        "\n",
        "## Time\n",
        "\n",
        "- Dropped rows with missing Time values (part of the 22 rows above)\n",
        "- These are critical for booking time calculations\n",
        "\n",
        "## Charge Description, Disposition Description, Charge Group Code, Charge Group Description\n",
        "\n",
        "- Used a multi-step approach that significantly reduced missing values:\n",
        "  1. Mapped from 274 frequent charges (≥100 occurrences) to fill 5,709 charge descriptions and codes\n",
        "  2. Used charge code prefixes to fill additional 17,153 values\n",
        "  3. Leveraged relationships between columns to fill more missing values\n",
        "  4. After all strategies, reduced missing values from ~32,000 to only ~9,300\n",
        "- Finally dropped the remaining rows with missing values (~2.8% of dataset), we chose to do this because filling out the rest of the missing values with things like the most common values would just be incorrect. Our approach fills out all the possible values with real connections between the columns.\n",
        "- This approach preserved data integrity while ensuring a complete dataset\n",
        "\n",
        "\n",
        "# NOTE\n",
        "\n",
        "- The following cell takes about ~3 minutes to run\n",
        "- The results of this code and cleaning can be used via the `clean_arrest_data.csv` file inside the `data` folder."
      ],
      "metadata": {
        "id": "aYYHrOfrRjkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_booking_date_and_time(df):\n",
        "    \"\"\"\n",
        "    Fix booking date and time together, properly handling day changes\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame with Time, Booking Time, Arrest Date, and Booking Date columns\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame with fixed Booking Date and Time\n",
        "    \"\"\"\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    if ('Booking Time' in clean_df.columns and 'Time' in clean_df.columns and\n",
        "        'Booking Date' in clean_df.columns and 'Arrest Date' in clean_df.columns):\n",
        "\n",
        "        # Count missing values\n",
        "        booking_time_na_count = clean_df['Booking Time'].isna().sum()\n",
        "        booking_date_na_count = clean_df['Booking Date'].isna().sum()\n",
        "        print(f\"Fixing {booking_time_na_count} missing 'Booking Time' and {booking_date_na_count} missing 'Booking Date' values\")\n",
        "\n",
        "        # Calculate typical time difference between arrest and booking for existing pairs\n",
        "        time_diffs = []\n",
        "        day_changes = 0\n",
        "        valid_times = clean_df.dropna(subset=['Time', 'Booking Time'])\n",
        "\n",
        "        for _, row in valid_times.iterrows():\n",
        "            arrest_time = float(row['Time'])\n",
        "            booking_time = float(row['Booking Time'])\n",
        "\n",
        "            # Convert to minutes for easier calculation\n",
        "            arrest_minutes = (int(arrest_time / 100) * 60) + (arrest_time % 100)\n",
        "            booking_minutes = (int(booking_time / 100) * 60) + (booking_time % 100)\n",
        "\n",
        "            # Calculate difference, accounting for day crossing\n",
        "            if booking_minutes < arrest_minutes:\n",
        "                # Booking is on the next day\n",
        "                diff_minutes = (24 * 60 - arrest_minutes) + booking_minutes\n",
        "                day_changes += 1\n",
        "            else:\n",
        "                diff_minutes = booking_minutes - arrest_minutes\n",
        "\n",
        "            time_diffs.append(diff_minutes)\n",
        "\n",
        "        # Calculate the median time difference (more robust than mean)\n",
        "        if time_diffs:\n",
        "            median_diff_minutes = np.median(time_diffs)\n",
        "            print(f\"Median time between arrest and booking: {median_diff_minutes:.1f} minutes\")\n",
        "            print(f\"Detected {day_changes} of {len(valid_times)} cases ({day_changes/len(valid_times)*100:.1f}%) where booking crosses to next day\")\n",
        "        else:\n",
        "            # Default to 2 hours if we can't calculate\n",
        "            median_diff_minutes = 120\n",
        "            print(\"Using default booking delay of 120 minutes\")\n",
        "\n",
        "        # First, fill any missing booking dates with arrest date (will be adjusted later if needed)\n",
        "        if booking_date_na_count > 0:\n",
        "            clean_df['Booking Date'] = clean_df['Booking Date'].fillna(clean_df['Arrest Date'])\n",
        "            print(f\"Initially filled {booking_date_na_count} missing 'Booking Date' values with 'Arrest Date'\")\n",
        "\n",
        "        # Process rows with missing booking times\n",
        "        date_adjustments = 0\n",
        "        for idx in clean_df[clean_df['Booking Time'].isna()].index:\n",
        "            arrest_time = clean_df.loc[idx, 'Time']\n",
        "\n",
        "            if pd.notna(arrest_time):\n",
        "                try:\n",
        "                    # Convert to float if it's not already\n",
        "                    arrest_time_float = float(arrest_time)\n",
        "\n",
        "                    # Convert to total minutes\n",
        "                    arrest_hours = int(arrest_time_float / 100)\n",
        "                    arrest_minutes = int(arrest_time_float % 100)\n",
        "                    total_arrest_minutes = arrest_hours * 60 + arrest_minutes\n",
        "\n",
        "                    # Add median difference\n",
        "                    total_booking_minutes = total_arrest_minutes + median_diff_minutes\n",
        "\n",
        "                    # Check if booking crosses to next day\n",
        "                    crosses_midnight = total_booking_minutes >= 24 * 60\n",
        "\n",
        "                    # Handle day overflow\n",
        "                    if crosses_midnight:\n",
        "                        total_booking_minutes -= 24 * 60\n",
        "\n",
        "                        # Also adjust the booking date if it crosses midnight\n",
        "                        if pd.notna(clean_df.loc[idx, 'Booking Date']) and pd.notna(clean_df.loc[idx, 'Arrest Date']):\n",
        "                            # Convert dates to datetime objects\n",
        "                            try:\n",
        "                                arrest_date = pd.to_datetime(clean_df.loc[idx, 'Arrest Date'])\n",
        "                                booking_date = arrest_date + pd.Timedelta(days=1)\n",
        "                                clean_df.loc[idx, 'Booking Date'] = booking_date.strftime('%Y-%m-%d')\n",
        "                                date_adjustments += 1\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error adjusting date: {e}\")\n",
        "\n",
        "                    # Convert back to military time float\n",
        "                    booking_hours = int(total_booking_minutes / 60)\n",
        "                    booking_minutes = int(total_booking_minutes % 60)\n",
        "                    booking_time = float(booking_hours * 100 + booking_minutes)\n",
        "\n",
        "                    clean_df.loc[idx, 'Booking Time'] = booking_time\n",
        "\n",
        "                except (ValueError, TypeError) as e:\n",
        "                    print(f\"Error processing time '{arrest_time}': {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Check for consistency in existing data - if booking time is earlier than arrest time but dates are the same\n",
        "        # This indicates a likely error in booking date\n",
        "        consistency_fixes = 0\n",
        "        for idx in clean_df.index:\n",
        "            arrest_time = clean_df.loc[idx, 'Time']\n",
        "            booking_time = clean_df.loc[idx, 'Booking Time']\n",
        "            arrest_date = clean_df.loc[idx, 'Arrest Date']\n",
        "            booking_date = clean_df.loc[idx, 'Booking Date']\n",
        "\n",
        "            if (pd.notna(arrest_time) and pd.notna(booking_time) and\n",
        "                pd.notna(arrest_date) and pd.notna(booking_date) and\n",
        "                arrest_date == booking_date):  # If dates are the same\n",
        "\n",
        "                # Check if booking time is earlier than arrest time (indicates day crossing)\n",
        "                if float(booking_time) < float(arrest_time):\n",
        "                    try:\n",
        "                        # Booking should be on the next day\n",
        "                        arrest_date_dt = pd.to_datetime(arrest_date)\n",
        "                        booking_date_new = arrest_date_dt + pd.Timedelta(days=1)\n",
        "                        clean_df.loc[idx, 'Booking Date'] = booking_date_new.strftime('%Y-%m-%d')\n",
        "                        consistency_fixes += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error fixing date consistency: {e}\")\n",
        "\n",
        "        if consistency_fixes > 0:\n",
        "            print(f\"Fixed {consistency_fixes} cases where booking time indicated next day but booking date was same as arrest date\")\n",
        "\n",
        "        # Check how many were filled\n",
        "        time_filled_count = booking_time_na_count - clean_df['Booking Time'].isna().sum()\n",
        "        print(f\"Successfully filled {time_filled_count} of {booking_time_na_count} missing 'Booking Time' values\")\n",
        "        print(f\"Adjusted {date_adjustments} booking dates to account for overnight bookings\")\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "def clean_la_crime_data_military_time(df):\n",
        "    \"\"\"\n",
        "    Comprehensive data cleaning with fix for military time format\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Original crime data DataFrame with military time as floats\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Fully cleaned DataFrame\n",
        "    \"\"\"\n",
        "    print(\"Starting data cleaning process with military time format fix...\")\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # Create a copy of the dataframe to avoid modifying the original\n",
        "    clean_df = df.copy()\n",
        "\n",
        "    # Step 1: Drop Cross Street Column\n",
        "    if 'Cross Street' in clean_df.columns:\n",
        "        clean_df = clean_df.drop(columns='Cross Street')\n",
        "        print(\"Dropped 'Cross Street' column\")\n",
        "\n",
        "    # Step 2: Drop rows with missing values in 'Time' or 'Arrest Type Code'\n",
        "    rows_before = len(clean_df)\n",
        "    clean_df = clean_df.dropna(subset=['Time', 'Arrest Type Code'])\n",
        "    rows_dropped = rows_before - len(clean_df)\n",
        "    print(f\"Dropped {rows_dropped} rows with missing 'Time' or 'Arrest Type Code'\")\n",
        "\n",
        "    # Step 3 & 4: Fix Booking Date and Time together, handling day changes\n",
        "    clean_df = fix_booking_date_and_time(clean_df)\n",
        "\n",
        "    # Step 5: Fill Booking Location based on Address, then Area Name as backup\n",
        "    if 'Booking Location' in clean_df.columns and 'Address' in clean_df.columns:\n",
        "        booking_loc_na_count = clean_df['Booking Location'].isna().sum()\n",
        "\n",
        "        # Create mapping from Address to most common Booking Location\n",
        "        address_to_booking = {}\n",
        "        valid_bookings = clean_df.dropna(subset=['Booking Location', 'Address'])\n",
        "\n",
        "        for address, group in valid_bookings.groupby('Address'):\n",
        "            most_common_location = group['Booking Location'].mode().iloc[0]\n",
        "            address_to_booking[address] = most_common_location\n",
        "\n",
        "        # Fill missing booking locations using Address\n",
        "        for idx in clean_df[clean_df['Booking Location'].isna()].index:\n",
        "            address = clean_df.loc[idx, 'Address']\n",
        "            if pd.notnull(address) and address in address_to_booking:\n",
        "                clean_df.loc[idx, 'Booking Location'] = address_to_booking[address]\n",
        "\n",
        "        # Check how many were filled by Address\n",
        "        filled_by_address = booking_loc_na_count - clean_df['Booking Location'].isna().sum()\n",
        "        print(f\"Filled {filled_by_address} missing 'Booking Location' values based on 'Address'\")\n",
        "\n",
        "        # For remaining missing values, use Area Name as backup\n",
        "        if 'Area Name' in clean_df.columns and clean_df['Booking Location'].isna().any():\n",
        "            remaining_before_area = clean_df['Booking Location'].isna().sum()\n",
        "\n",
        "            # Create mapping from Area Name to most common Booking Location\n",
        "            area_to_booking = {}\n",
        "            area_bookings = clean_df.dropna(subset=['Booking Location', 'Area Name'])\n",
        "\n",
        "            for area, group in area_bookings.groupby('Area Name'):\n",
        "                most_common_location = group['Booking Location'].mode().iloc[0]\n",
        "                area_to_booking[area] = most_common_location\n",
        "\n",
        "            # Fill missing booking locations using Area Name\n",
        "            for idx in clean_df[clean_df['Booking Location'].isna()].index:\n",
        "                area = clean_df.loc[idx, 'Area Name']\n",
        "                if pd.notnull(area) and area in area_to_booking:\n",
        "                    clean_df.loc[idx, 'Booking Location'] = area_to_booking[area]\n",
        "\n",
        "            # Check how many were filled by Area Name\n",
        "            filled_by_area = remaining_before_area - clean_df['Booking Location'].isna().sum()\n",
        "            print(f\"Filled {filled_by_area} additional missing 'Booking Location' values based on 'Area Name'\")\n",
        "\n",
        "    # Step 6: Fill Booking Location Code based on Booking Location\n",
        "    if 'Booking Location Code' in clean_df.columns and 'Booking Location' in clean_df.columns:\n",
        "        booking_code_na_count = clean_df['Booking Location Code'].isna().sum()\n",
        "\n",
        "        # Create mapping from Booking Location to Booking Location Code\n",
        "        location_to_code = {}\n",
        "        valid_codes = clean_df.dropna(subset=['Booking Location', 'Booking Location Code'])\n",
        "\n",
        "        for location, group in valid_codes.groupby('Booking Location'):\n",
        "            most_common_code = group['Booking Location Code'].mode().iloc[0]\n",
        "            location_to_code[location] = most_common_code\n",
        "\n",
        "        # Fill missing booking location codes\n",
        "        for idx in clean_df[clean_df['Booking Location Code'].isna()].index:\n",
        "            location = clean_df.loc[idx, 'Booking Location']\n",
        "            if pd.notnull(location) and location in location_to_code:\n",
        "                clean_df.loc[idx, 'Booking Location Code'] = location_to_code[location]\n",
        "\n",
        "        # Check how many were filled\n",
        "        filled_count = booking_code_na_count - clean_df['Booking Location Code'].isna().sum()\n",
        "        print(f\"Filled {filled_count} missing 'Booking Location Code' values based on 'Booking Location'\")\n",
        "\n",
        "    # Step 7: Fill charge-related columns - first using frequent charges\n",
        "    charge_columns = ['Charge Group Code', 'Charge Group Description', 'Charge Description', 'Disposition Description']\n",
        "\n",
        "    # Filter to focus on charges that appear at least 100 times\n",
        "    charge_counts = clean_df['Charge'].value_counts()\n",
        "    frequent_charges = charge_counts[charge_counts >= 100].index\n",
        "\n",
        "    print(f\"Using {len(frequent_charges)} frequent charges (appearing ≥100 times) out of {len(charge_counts)} total charges\")\n",
        "\n",
        "    # Create mappings for each charge-related column\n",
        "    charge_mappings = {}\n",
        "\n",
        "    for column in charge_columns:\n",
        "        if column in clean_df.columns:\n",
        "            # Create mapping from 'Charge' to this column\n",
        "            mapping = {}\n",
        "            valid_data = clean_df[clean_df['Charge'].isin(frequent_charges)].dropna(subset=['Charge', column])\n",
        "\n",
        "            for charge, group in valid_data.groupby('Charge'):\n",
        "                most_common_value = group[column].mode().iloc[0]\n",
        "                mapping[charge] = most_common_value\n",
        "\n",
        "            charge_mappings[column] = mapping\n",
        "            print(f\"Created mapping from 'Charge' to '{column}' with {len(mapping)} unique mappings\")\n",
        "\n",
        "    # Apply mappings to fill missing values\n",
        "    for column in charge_columns:\n",
        "        if column in charge_mappings:\n",
        "            column_na_count = clean_df[column].isna().sum()\n",
        "\n",
        "            # Only apply to rows with frequent charges\n",
        "            for idx in clean_df[(clean_df[column].isna()) & (clean_df['Charge'].isin(frequent_charges))].index:\n",
        "                charge = clean_df.loc[idx, 'Charge']\n",
        "                if charge in charge_mappings[column]:\n",
        "                    clean_df.loc[idx, column] = charge_mappings[column][charge]\n",
        "\n",
        "            # Check how many were filled\n",
        "            filled_count = column_na_count - clean_df[column].isna().sum()\n",
        "            print(f\"Filled {filled_count} missing '{column}' values based on 'Charge'\")\n",
        "\n",
        "    # Step 8: Use charge prefixes for less common charges\n",
        "    print(\"\\nHandling less frequent charges...\")\n",
        "\n",
        "    # Create charge prefixes\n",
        "    if 'Charge' in clean_df.columns:\n",
        "        # Extract prefix from charge code - looking for patterns like \"PC\", \"HS\", \"VC\", etc.\n",
        "        clean_df['Charge_Prefix'] = clean_df['Charge'].str.extract(r'([A-Za-z]{1,3})(?:\\d|\\(|$)')[0].str.upper()\n",
        "\n",
        "        for column in charge_columns:\n",
        "            if column in clean_df.columns and clean_df[column].isna().any():\n",
        "                before_count = clean_df[column].isna().sum()\n",
        "\n",
        "                # Create mapping from prefix to most common value\n",
        "                prefix_to_value = {}\n",
        "                valid_data = clean_df.dropna(subset=[column, 'Charge_Prefix'])\n",
        "\n",
        "                for prefix, group in valid_data.groupby('Charge_Prefix'):\n",
        "                    if len(group) >= 5 and pd.notna(prefix):  # Only use prefixes with sufficient examples\n",
        "                        most_common = group[column].mode().iloc[0]\n",
        "                        prefix_to_value[prefix] = most_common\n",
        "\n",
        "                # Apply mapping\n",
        "                for idx in clean_df[clean_df[column].isna()].index:\n",
        "                    prefix = clean_df.loc[idx, 'Charge_Prefix']\n",
        "                    if pd.notna(prefix) and prefix in prefix_to_value:\n",
        "                        clean_df.loc[idx, column] = prefix_to_value[prefix]\n",
        "\n",
        "                filled = before_count - clean_df[column].isna().sum()\n",
        "                print(f\"Filled {filled} missing '{column}' values using charge code prefixes\")\n",
        "\n",
        "        # Remove the temporary column\n",
        "        clean_df = clean_df.drop(columns=['Charge_Prefix'])\n",
        "\n",
        "    # Step 9: Use relationships between charge columns\n",
        "    column_relationships = [\n",
        "        ('Charge Group Code', 'Charge Group Description'),\n",
        "        ('Charge Group Code', 'Charge Description'),\n",
        "        ('Charge Group Description', 'Charge Group Code'),\n",
        "        ('Charge Group Description', 'Charge Description')\n",
        "    ]\n",
        "\n",
        "    print(\"\\nUsing relationships between columns...\")\n",
        "\n",
        "    for source_col, target_col in column_relationships:\n",
        "        if source_col in clean_df.columns and target_col in clean_df.columns:\n",
        "            if clean_df[target_col].isna().any():\n",
        "                before_count = clean_df[target_col].isna().sum()\n",
        "\n",
        "                # Create mapping\n",
        "                col_to_col = {}\n",
        "                valid_data = clean_df.dropna(subset=[source_col, target_col])\n",
        "\n",
        "                for val, group in valid_data.groupby(source_col):\n",
        "                    most_common = group[target_col].mode().iloc[0]\n",
        "                    col_to_col[val] = most_common\n",
        "\n",
        "                # Apply mapping\n",
        "                for idx in clean_df[(clean_df[target_col].isna()) & (clean_df[source_col].notna())].index:\n",
        "                    val = clean_df.loc[idx, source_col]\n",
        "                    if val in col_to_col:\n",
        "                        clean_df.loc[idx, target_col] = col_to_col[val]\n",
        "\n",
        "                filled = before_count - clean_df[target_col].isna().sum()\n",
        "                if filled > 0:\n",
        "                    print(f\"Filled {filled} missing '{target_col}' values using '{source_col}'\")\n",
        "\n",
        "\n",
        "    # Final count of missing values\n",
        "    print(\"\\nRemaining missing values after cleaning:\")\n",
        "    print(clean_df.isna().sum())\n",
        "\n",
        "    return clean_df\n",
        "\n",
        "clean_df = clean_la_crime_data_military_time(df)\n",
        "\n",
        "# Final step: Drop remaining rows with missing values\n",
        "before_drop = len(clean_df)\n",
        "clean_df = clean_df.dropna(subset=['Charge Group Code', 'Charge Group Description', 'Charge Description', 'Disposition Description'])\n",
        "after_drop = len(clean_df)\n",
        "print(f\"Dropped {before_drop - after_drop} rows with remaining missing values\")\n",
        "print(f\"\\nFinal dataset shape: {clean_df.shape}\")\n",
        "\n",
        "# Extra\n",
        "\n",
        "# Filtered to only look at adults (people over the age of 18)\n",
        "# About 4k rows had LAT and LON values of 0, so we removed those rows as well.\n",
        "clean_df = clean_df[clean_df['Age'] >= 18]\n",
        "clean_df = clean_df[(clean_df['LAT'] != 0) & (clean_df['LON'] != 0)]"
      ],
      "metadata": {
        "id": "nQKmN8hakaBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d007e1b-acf1-4503-d754-ac1aa4a6108a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data cleaning process with military time format fix...\n",
            "Original dataset shape: (334577, 25)\n",
            "Dropped 'Cross Street' column\n",
            "Dropped 22 rows with missing 'Time' or 'Arrest Type Code'\n",
            "Fixing 75378 missing 'Booking Time' and 75378 missing 'Booking Date' values\n",
            "Median time between arrest and booking: 157.0 minutes\n",
            "Detected 41699 of 259177 cases (16.1%) where booking crosses to next day\n",
            "Initially filled 75378 missing 'Booking Date' values with 'Arrest Date'\n",
            "Fixed 24 cases where booking time indicated next day but booking date was same as arrest date\n",
            "Successfully filled 75378 of 75378 missing 'Booking Time' values\n",
            "Adjusted 6576 booking dates to account for overnight bookings\n",
            "Filled 72932 missing 'Booking Location' values based on 'Address'\n",
            "Filled 2487 additional missing 'Booking Location' values based on 'Area Name'\n",
            "Filled 75382 missing 'Booking Location Code' values based on 'Booking Location'\n",
            "Using 274 frequent charges (appearing ≥100 times) out of 4918 total charges\n",
            "Created mapping from 'Charge' to 'Charge Group Code' with 232 unique mappings\n",
            "Created mapping from 'Charge' to 'Charge Group Description' with 232 unique mappings\n",
            "Created mapping from 'Charge' to 'Charge Description' with 232 unique mappings\n",
            "Created mapping from 'Charge' to 'Disposition Description' with 274 unique mappings\n",
            "Filled 5709 missing 'Charge Group Code' values based on 'Charge'\n",
            "Filled 5709 missing 'Charge Group Description' values based on 'Charge'\n",
            "Filled 5709 missing 'Charge Description' values based on 'Charge'\n",
            "Filled 28715 missing 'Disposition Description' values based on 'Charge'\n",
            "\n",
            "Handling less frequent charges...\n",
            "Filled 17153 missing 'Charge Group Code' values using charge code prefixes\n",
            "Filled 17156 missing 'Charge Group Description' values using charge code prefixes\n",
            "Filled 17153 missing 'Charge Description' values using charge code prefixes\n",
            "Filled 1730 missing 'Disposition Description' values using charge code prefixes\n",
            "\n",
            "Using relationships between columns...\n",
            "\n",
            "Filling remaining missing values...\n",
            "Still missing 9256 values from 'Charge Group Code'\n",
            "Still missing 9304 values from 'Charge Group Description'\n",
            "Still missing 9256 values from 'Charge Description'\n",
            "Still missing 77 values from 'Disposition Description'\n",
            "\n",
            "Remaining missing values after cleaning:\n",
            "Report ID                      0\n",
            "Report Type                    0\n",
            "Arrest Date                    0\n",
            "Time                           0\n",
            "Area ID                        0\n",
            "Area Name                      0\n",
            "Reporting District             0\n",
            "Age                            0\n",
            "Sex Code                       0\n",
            "Descent Code                   0\n",
            "Charge Group Code           9256\n",
            "Charge Group Description    9304\n",
            "Arrest Type Code               0\n",
            "Charge                         0\n",
            "Charge Description          9256\n",
            "Disposition Description       77\n",
            "Address                        0\n",
            "LAT                            0\n",
            "LON                            0\n",
            "Location                       0\n",
            "Booking Date                   0\n",
            "Booking Time                   0\n",
            "Booking Location               0\n",
            "Booking Location Code          0\n",
            "dtype: int64\n",
            "Dropped 9339 rows with remaining missing values\n",
            "\n",
            "Final dataset shape: (325216, 24)\n"
          ]
        }
      ]
    }
  ]
}